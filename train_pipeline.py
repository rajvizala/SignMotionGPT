"""
Main training pipeline for Motion LLM (Matched to test_overfit.py logic)
Run this script to execute the full training process matching the reference implementation.
"""
import os
import random
import torch
import json
import argparse
from types import SimpleNamespace
import warnings
from typing import Optional, List, Dict, Any, Tuple
from collections import defaultdict
import numpy as np

# Import updated modules
from config import (
    SEED, DATA_JSON_PATH, MODEL_NAME, PIPELINE_OUTPUT_DIR,
    HF_STAGE1_REPO_ID, HF_STAGE2_REPO_ID, HF_STAGE2_SAVE_SUBDIR,
    HF_STAGE3_REPO_ID, HF_STAGE3_SAVE_SUBDIR,
    FORCE_STAGE2_FROM_STAGE1, HF_USE_HUB, HF_TOKEN,
    EVALUATION_WORDS, EVAL_SAMPLE_LIMIT, RUN_EVALS_ONLY,
    TEST_EVAL_OUTPUT_DIR, TEST_EVAL_DOWNLOAD_DIR, TEST_EVAL_EXTRACT_DIR,
    TEST_EVAL_SAMPLE_LIMIT, TEST_EVAL_MAX_ZIPS, TEST_EVAL_HF_REPO, TEST_EVAL_HF_SUBFOLDER
)
from data import read_json_data, deduplicate_and_prepare_data, build_motion_vocab
from model import setup_model_and_tokenizer_raw, ensure_tokenizer_has_motion_tokens
from train import (
    train_stage1_raw, train_stage2_raw, train_stage3_instruct_raw, resolve_and_ensure_repo,
    repo_has_stage_latest, load_model_and_tokenizer_from_hf,
    download_training_state, repo_get_latest_epoch_subfolder,
    load_model_and_tokenizer_from_hf_subfolder, download_training_state_from_subfolder
)
from metrics import (
    evaluate_metrics_encoder_style, run_inference_on_all_samples,
    evaluate_metrics_motiongpt_style, save_side_by_side_visualizations,
    evaluate_stage3_multiref_encoder_style,  # <-- add this
)
import test_dataset_eval

# Suppress warnings
warnings.filterwarnings("ignore")


def parse_args():
    p = argparse.ArgumentParser(description="Train SignMotionGPT pipeline stages (1/2/3) with HF resume support.")
    p.add_argument(
        "--stage",
        type=str,
        default=os.environ.get("TRAIN_STAGE", "all"),
        choices=["1", "2", "3", "all"],
        help="Which stage(s) to run: 1, 2, 3, or all (default: all).",
    )
    p.add_argument("--skip-eval", action="store_true", help="Skip evaluation/metrics after training.")
    p.add_argument("--skip-test-eval", action="store_true", help="Skip held-out test dataset evaluation step.")
    return p.parse_args()


def _load_stage_from_hf(resolved_repo: str, stage_subdir: str):
    """
    Best-effort load of (model, tokenizer, start_epoch) from HF for a given stage folder.
    Supports both {stage}/latest and fallback {stage}/epoch-XXX if latest is missing.
    """
    start_epoch = 0
    loaded = None
    if not resolved_repo:
        return None, 0

    if repo_has_stage_latest(resolved_repo, stage_subdir, HF_TOKEN):
        loaded = load_model_and_tokenizer_from_hf(resolved_repo, stage_subdir, HF_TOKEN)
        state = download_training_state(resolved_repo, stage_subdir, HF_TOKEN)
        if state and isinstance(state.get("epoch_completed"), int):
            start_epoch = state["epoch_completed"]
        return loaded, start_epoch

    latest_sub = repo_get_latest_epoch_subfolder(resolved_repo, stage_subdir, HF_TOKEN)
    if latest_sub:
        loaded = load_model_and_tokenizer_from_hf_subfolder(resolved_repo, latest_sub, HF_TOKEN)
        state = download_training_state_from_subfolder(resolved_repo, latest_sub, HF_TOKEN)
        if state and isinstance(state.get("epoch_completed"), int):
            start_epoch = state["epoch_completed"]
    return loaded, start_epoch

def main():
    """Main function to run the entire pipeline matching test_overfit.py."""
    args = parse_args()
    print("="*80)
    print("      Motion LLM Training Pipeline (Matches test_overfit.py)")
    print("="*80)

    # Set seeds
    random.seed(SEED)
    torch.manual_seed(SEED)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(SEED)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # 1. Load ALL data
    print(f"\n[1/6] Loading dataset from {DATA_JSON_PATH}...")
    all_entries = read_json_data(DATA_JSON_PATH)
    
    # 2. Clean the ENTIRE dataset and get all tokens
    print("\n[2/6] Cleaning dataset...")
    cleaned_data, all_motion_tokens = deduplicate_and_prepare_data(all_entries)
    unique_words = sorted({str(item.get("word", "")).lower().strip() for item in cleaned_data if str(item.get("word", "")).strip()})
    print(f"\nüìå Unique words in cleaned dataset: {len(unique_words)}")

    do_s1 = args.stage in ("1", "all")
    do_s2 = args.stage in ("2", "all")
    do_s3 = args.stage in ("3", "all")

    # We'll track the "current" model/tokenizer as we progress through stages
    current_model = None
    tokenizer = None

    # 3. Stage 1: Initialize or resume from HF, then train (optional)
    motion_model = None
    resolved_stage1_repo = resolve_and_ensure_repo(HF_STAGE1_REPO_ID, HF_TOKEN) if HF_USE_HUB else None
    if do_s1:
        print("\n[3/6] Stage 1 Setup & Training...")
        stage1_loaded, start_epoch_s1 = _load_stage_from_hf(resolved_stage1_repo, "stage1")
        if stage1_loaded:
            base_model, tokenizer = stage1_loaded
            added = ensure_tokenizer_has_motion_tokens(tokenizer, all_motion_tokens)
            if added > 0:
                base_model.resize_token_embeddings(len(tokenizer))
        else:
            base_model, tokenizer = setup_model_and_tokenizer_raw(MODEL_NAME, all_motion_tokens)

        print(f"\nStarting Stage 1 training on {len(cleaned_data)} samples (resume from epoch {start_epoch_s1}).")
        motion_model = train_stage1_raw(
            base_model,
            tokenizer,
            cleaned_data,
            device,
            start_epoch=start_epoch_s1,
            hf_repo_id=resolved_stage1_repo,
        )
        current_model = motion_model
    else:
        # If Stage 1 isn't being trained, set a safe baseline for downstream stages.
        # We'll prefer downstream checkpoints when available (Stage2/Stage3 loaders below).
        current_model, tokenizer = setup_model_and_tokenizer_raw(MODEL_NAME, all_motion_tokens)

    # 4. Stage 2: Initialize or resume from HF, then train
    final_model = current_model
    last_trained_stage = "stage1" if do_s1 else "base"

    resolved_stage2_repo = resolve_and_ensure_repo(HF_STAGE2_REPO_ID, HF_TOKEN) if HF_USE_HUB else None
    if do_s2:
        print("\n[4/6] Stage 2 Setup & Training...")
        print(f"Stage 2 resume policy: FORCE_STAGE2_FROM_STAGE1={FORCE_STAGE2_FROM_STAGE1}, save_subdir='{HF_STAGE2_SAVE_SUBDIR}'")

        stage2_loaded, start_epoch_s2 = (None, 0)
        if not FORCE_STAGE2_FROM_STAGE1 and resolved_stage2_repo:
            stage2_loaded, start_epoch_s2 = _load_stage_from_hf(resolved_stage2_repo, HF_STAGE2_SAVE_SUBDIR)

        if stage2_loaded:
            stage2_model, tokenizer = stage2_loaded
            added2 = ensure_tokenizer_has_motion_tokens(tokenizer, all_motion_tokens)
            if added2 > 0:
                stage2_model.resize_token_embeddings(len(tokenizer))
        else:
            stage2_model = motion_model if motion_model is not None else current_model

        print(f"\nStarting Stage 2 training on {len(cleaned_data)} samples (resume from epoch {start_epoch_s2}).")
        final_model = train_stage2_raw(
            stage2_model,
            tokenizer,
            cleaned_data,
            device,
            start_epoch=start_epoch_s2,
            hf_repo_id=resolved_stage2_repo,
            hf_stage_subdir=HF_STAGE2_SAVE_SUBDIR,
        )
        last_trained_stage = "stage2"

    # 4.5. Stage 3: Instruct tuning (optional)
    resolved_stage3_repo = resolve_and_ensure_repo(HF_STAGE3_REPO_ID, HF_TOKEN) if HF_USE_HUB else None
    if do_s3:
        print("\n[4.5/6] Stage 3 Setup & Training (Instruct)...")
        stage3_loaded, start_epoch_s3 = (None, 0)
        if resolved_stage3_repo:
            stage3_loaded, start_epoch_s3 = _load_stage_from_hf(resolved_stage3_repo, HF_STAGE3_SAVE_SUBDIR)

        if stage3_loaded:
            stage3_model, tokenizer = stage3_loaded
            added3 = ensure_tokenizer_has_motion_tokens(tokenizer, all_motion_tokens)
            if added3 > 0:
                stage3_model.resize_token_embeddings(len(tokenizer))
        else:
            # Fallback chain: Stage 2 (already trained in this run) -> Stage 2 from HF -> current_model
            stage3_model = final_model
            if (not do_s2) and resolved_stage2_repo:
                stage2_fallback_loaded, _ = _load_stage_from_hf(resolved_stage2_repo, HF_STAGE2_SAVE_SUBDIR)
                if stage2_fallback_loaded:
                    stage3_model, tokenizer = stage2_fallback_loaded
                    added3b = ensure_tokenizer_has_motion_tokens(tokenizer, all_motion_tokens)
                    if added3b > 0:
                        stage3_model.resize_token_embeddings(len(tokenizer))

        print(f"\nStarting Stage 3 training (instruct) (resume from epoch {start_epoch_s3}).")
        final_model = train_stage3_instruct_raw(
            stage3_model,
            tokenizer,
            cleaned_data,
            device,
            start_epoch=start_epoch_s3,
            hf_repo_id=resolved_stage3_repo,
            hf_stage_subdir=HF_STAGE3_SAVE_SUBDIR,
        )
        last_trained_stage = "stage3"
    
    # Save final model locally
    if not os.path.exists(PIPELINE_OUTPUT_DIR):
        os.makedirs(PIPELINE_OUTPUT_DIR)
    final_model.save_pretrained(PIPELINE_OUTPUT_DIR)
    tokenizer.save_pretrained(PIPELINE_OUTPUT_DIR)
    print(f"Model saved to {PIPELINE_OUTPUT_DIR}")

    # 5. Evaluation on Specific Words
    if args.skip_eval:
        print("\n[5/6] Evaluation skipped (--skip-eval).")
        evaluation_data = []
    else:
        print("\n[5/6] Evaluation on Specific Words...")
        print("--- Filtering data for evaluation on specific words ---")
        evaluation_data = [item for item in cleaned_data if item['word'].lower() in EVALUATION_WORDS]
        print(f"Found {len(evaluation_data)} samples for evaluation words: {EVALUATION_WORDS}")

    metrics_json_path = os.path.join(PIPELINE_OUTPUT_DIR, "metrics.json")

    # 6. Metrics-only mode or full flow
    include_participant_in_prompt = (last_trained_stage != "stage3")

    # Stage 3: use the new multi-ref encoder-only eval
    if (not args.skip_eval) and (last_trained_stage == "stage3"):
        stage3_metrics = evaluate_stage3_multiref_encoder_style(
            final_model,
            tokenizer,
            evaluation_data,
            device,
            k_samples=10,
            sample_limit=None,  # use all eval rows for those words
        )

        os.makedirs(PIPELINE_OUTPUT_DIR, exist_ok=True)
        with open(metrics_json_path, "w", encoding="utf-8") as f:
            json.dump(stage3_metrics, f, ensure_ascii=False, indent=2)
        print(f"\n‚úÖ Saved Stage 3 multi-ref metrics to {metrics_json_path}")

        if RUN_EVALS_ONLY:
            return

        # Visualization: one GT (closest ref) vs GEN (best-of-K) per word
        viz_dir = os.path.join(PIPELINE_OUTPUT_DIR, "html_visualizations")
        save_side_by_side_visualizations(stage3_metrics.get("pairs_closest", []), viz_dir, limit=4)

    # Stage 1/2: keep existing eval behavior
    elif (not args.skip_eval) and RUN_EVALS_ONLY:
        metrics_enc = evaluate_metrics_encoder_style(
            final_model, tokenizer, evaluation_data, device,
            sample_limit=EVAL_SAMPLE_LIMIT,
            include_participant=include_participant_in_prompt,
        )
        os.makedirs(PIPELINE_OUTPUT_DIR, exist_ok=True)
        metrics_payload = {
            "source": "vqvae_encoder",
            "fid": metrics_enc.get("fid"),
            "diversity": {
                "ground_truth": metrics_enc.get("diversity_gt"),
                "model": metrics_enc.get("diversity_gen"),
            },
            "multimodality": {
                "ground_truth": metrics_enc.get("mim_gt"),
                "model": metrics_enc.get("mim_gen"),
            },
            "num_pairs": len(metrics_enc.get("pairs", [])),
        }
        with open(metrics_json_path, "w", encoding="utf-8") as f:
            json.dump(metrics_payload, f, ensure_ascii=False, indent=2)
        print(f"\n‚úÖ Saved metrics to {metrics_json_path}")
        return

    elif not args.skip_eval:
        run_inference_on_all_samples(final_model, tokenizer, evaluation_data, device, include_participant=include_participant_in_prompt)
        metrics_token = evaluate_metrics_motiongpt_style(
            final_model, tokenizer, evaluation_data, all_motion_tokens, device, include_participant=include_participant_in_prompt
        )
        metrics_enc = evaluate_metrics_encoder_style(
            final_model, tokenizer, evaluation_data, device, sample_limit=EVAL_SAMPLE_LIMIT, include_participant=include_participant_in_prompt
        )
        viz_dir = os.path.join(PIPELINE_OUTPUT_DIR, "html_visualizations")
        save_side_by_side_visualizations(metrics_token["pairs"], viz_dir, limit=4)

    # 7. Run Test Dataset Evaluation (test_dataset_eval.py)
    if args.skip_test_eval:
        print("\n[6/6] Held-out test dataset evaluation skipped (--skip-test-eval).")
    else:
        print("\n[6/6] Running Evaluation on Held-out Test Dataset...")
    try:
        # Construct args matching test_dataset_eval.parse_args
        eval_args = SimpleNamespace(
            drive_url=None,
            drive_id=None,
            local_extracted_dir=None, # Will assume user needs to configure this or it uses defaults if not provided
            # Note: test_dataset_eval requires one of drive/local. We can try to rely on defaults or skip if not configured.
            # We will set download_dir and extract_dir from config.
            download_dir=TEST_EVAL_DOWNLOAD_DIR,
            extract_dir=TEST_EVAL_EXTRACT_DIR,
            max_zips=TEST_EVAL_MAX_ZIPS,
            hf_repo_id=TEST_EVAL_HF_REPO,
            hf_subfolder=TEST_EVAL_HF_SUBFOLDER,
            vqvae_ckpt=None,
            stats_path=None,
            output_dir=TEST_EVAL_OUTPUT_DIR,
            sample_limit=TEST_EVAL_SAMPLE_LIMIT,
            seed=SEED
        )
        
        # For this pipeline, we might want to pass the *currently loaded* model instead of reloading from HF?
        # test_dataset_eval.run_evaluation loads from HF. 
        # The prompt asked to "incorporate... code of test_dataset_eval.py".
        # Ideally we pass the model object, but run_evaluation is written to load from HF.
        # Given we just saved and pushed (if enabled), loading from HF is fine. 
        # If we haven't pushed (HF_USE_HUB=False), run_evaluation might fail if it tries to load from HF.
        # However, the prompt implies using test_overfit.py training setup which pushes to HF.
        
        # Critical fix: If we want to use the *local* model we just trained, we should modify test_dataset_eval or pass it.
        # But test_dataset_eval.run_evaluation doesn't accept model arg.
        # For now, we'll attempt to run it as designed (loading from HF).
        # If HF_USE_HUB is False, this step might fail.
        
        # Let's check if we can use local_extracted_dir if it exists, otherwise drive download.
        # We will use a try-except block.
        
        if args.skip_test_eval:
            eval_args = None
        else:
            print("Calling test_dataset_eval.run_evaluation...")
        # We need to provide either drive-url/id or local-extracted. 
        # We'll try to use the extracted dir if it has content, otherwise default to download if URL known?
        # Actually, since we don't have a drive URL in config (it was an arg), we might skip this if not set up?
        # But the user said "include the code".
        
        # We'll default to using the extract dir if it exists, otherwise we might need to ask or skip.
        # Let's assume the user has data or we use the default drive-id if known (it wasn't in the provided file).
        # Wait, test_dataset_eval.py has mutually exclusive required group.
        # I'll add a fallback: if TEST_EVAL_EXTRACT_DIR exists and has files, use it.
        
        if os.path.exists(TEST_EVAL_EXTRACT_DIR) and os.listdir(TEST_EVAL_EXTRACT_DIR):
             eval_args.local_extracted_dir = TEST_EVAL_EXTRACT_DIR
        else:
             # We don't have a drive URL hardcoded. 
             # We will mock the arg to fail gracefully or print a message.
             print("‚ö†Ô∏è  Skipping test_dataset_eval: No local data found and no Drive URL configured.")
             eval_args = None

        if eval_args and (not args.skip_test_eval):
            test_dataset_eval.run_evaluation(eval_args)
            
    except Exception as e:
        print(f"‚ö†Ô∏è  Test dataset evaluation failed: {e}")

    print("\n" + "="*60)
    print("Training pipeline complete!")
    print("="*60)
    print(f"Models saved to: {PIPELINE_OUTPUT_DIR}")

if __name__ == "__main__":
    main()
